# Big Data Product Analysis with PySpark

This project leverages Apache Spark (PySpark) to perform large-scale data analysis on product-related data using both RDD and DataFrame APIs.

## Project Structure

- `BIG_DATA_project_2 (1).ipynb`: Jupyter notebook containing PySpark setup, data loading, RDD/DataFrame transformations, and visualizations.

## Dataset

- **Filename**: `product.csv`
- **Source**: Google Drive (mounted in Colab)
- **Content**: Product-level sales or inventory data

##  Objectives

- Load product data using Spark DataFrame
- Convert DataFrame to RDD and explore RDD transformations
- Perform data cleaning and feature extraction
- Generate basic summaries and statistics
- Visualize key trends using `matplotlib` and `seaborn`

##  Tools & Libraries

- `PySpark` – Distributed data processing
- `RDD` – Low-level resilient distributed dataset operations
- `Spark DataFrame` – High-level structured data API
- `pandas`, `numpy` – Data manipulation
- `matplotlib`, `seaborn` – Visualizations

##  Highlights

- Efficient data processing using PySpark on large datasets
- Demonstrates the difference between RDD and DataFrame usage
- Visual insight into product distribution and metrics

##  How to Run

1. Open in Google Colab or local Jupyter environment with PySpark:
   ```bash
   jupyter notebook "BIG_DATA_project_2 (1).ipynb"
   ```
2. Make sure `product.csv` is accessible in the provided path (`/content/drive/...`).
3. Run all cells to reproduce the analysis.

---

*Built for scalable data analysis using Spark*
