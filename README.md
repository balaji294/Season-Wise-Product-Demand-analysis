# ğŸš€ Big Data Product Analysis with PySpark

This project leverages Apache Spark (PySpark) to perform large-scale data analysis on product-related data using both RDD and DataFrame APIs.

## ğŸ“ Project Structure

- `BIG_DATA_project_2 (1).ipynb`: Jupyter notebook containing PySpark setup, data loading, RDD/DataFrame transformations, and visualizations.

## ğŸ“Š Dataset

- **Filename**: `product.csv`
- **Source**: Google Drive (mounted in Colab)
- **Content**: Product-level sales or inventory data

## ğŸ§  Objectives

- Load product data using Spark DataFrame
- Convert DataFrame to RDD and explore RDD transformations
- Perform data cleaning and feature extraction
- Generate basic summaries and statistics
- Visualize key trends using `matplotlib` and `seaborn`

## ğŸ› ï¸ Tools & Libraries

- `PySpark` â€“ Distributed data processing
- `RDD` â€“ Low-level resilient distributed dataset operations
- `Spark DataFrame` â€“ High-level structured data API
- `pandas`, `numpy` â€“ Data manipulation
- `matplotlib`, `seaborn` â€“ Visualizations

## ğŸ“Œ Highlights

- Efficient data processing using PySpark on large datasets
- Demonstrates the difference between RDD and DataFrame usage
- Visual insight into product distribution and metrics

## ğŸš€ How to Run

1. Open in Google Colab or local Jupyter environment with PySpark:
   ```bash
   jupyter notebook "BIG_DATA_project_2 (1).ipynb"
   ```
2. Make sure `product.csv` is accessible in the provided path (`/content/drive/...`).
3. Run all cells to reproduce the analysis.

---

ğŸ“Š *Built for scalable data analysis using Spark*
